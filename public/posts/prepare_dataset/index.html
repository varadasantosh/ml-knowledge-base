<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>My ML Portfolio</title>
<meta name="keywords" content="">
<meta name="description" content="
!pip install datasets
!pip install huggingface_hub
Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)
Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)
Requirement already satisfied: pyarrow&gt;=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)
Requirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)
Requirement already satisfied: requests&gt;=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)
Requirement already satisfied: tqdm&gt;=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)
Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess&lt;0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec&lt;=2024.12.0,&gt;=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]&lt;=2024.12.0,&gt;=2023.1.0-&gt;datasets) (2024.12.0)
Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)
Requirement already satisfied: huggingface-hub&gt;=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)
Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)
Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (2.6.1)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.3.2)
Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (25.3.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.5.0)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (6.2.0)
Requirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (0.3.0)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.18.3)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub&gt;=0.24.0-&gt;datasets) (4.12.2)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.4.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2.3.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2025.1.31)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2.8.2)
Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.1)
Requirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.1)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.17.0)
Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)
Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)
Requirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)
Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)
Requirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;huggingface_hub) (3.4.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;huggingface_hub) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;huggingface_hub) (2.3.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;huggingface_hub) (2025.1.31)

Import Libraries
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace
from datasets import load_dataset
Define Generator for Training Tokenizer
def get_ds(ds,lang):
  for sentence in ds[&#39;translation&#39;]:
    yield sentence[lang]
Build Tokenizer
def build_tokenizer(ds,lang):
  tokenizer = Tokenizer(WordLevel(unk_token=&#34;[UNK]&#34;))
  tokenizer.pre_tokenizer = Whitespace()
  trainer = WordLevelTrainer(special_tokens=[&#34;[UNK]&#34;,&#34;[SOS]&#34;,&#34;[EOS]&#34;,&#34;[PAD]&#34;],min_frequency=2)
  tokenizer.train_from_iterator(get_ds(ds,lang),trainer)
  return tokenizer
Train the Tokenizer on Dataset
raw_ds = load_dataset(&#39;opus_books&#39;,&#39;en-fr&#39;,split=&#39;train&#39;)
src_tokenizer = build_tokenizer(raw_ds,&#39;en&#39;)
tgt_tokenizer = build_tokenizer(raw_ds,&#39;fr&#39;)
Understand the Tokenizer Behaviour
src_tokenizer.get_vocab_size()
30000

from collections import OrderedDict
vocab = src_tokenizer.get_vocab()
sorted_vocab = sorted(vocab.items(),key = lambda x:x[1])
ordered_vocab=OrderedDict(sorted_vocab)
example_sentence = raw_ds[&#39;translation&#39;][5][&#39;en&#39;]
tokens= src_tokenizer.encode(example_sentence).tokens
token_ids = src_tokenizer.encode(example_sentence).ids
id_token_pair = [f&#34;{(token,ordered_vocab[token])}&#34; for token in tokens]
print(f&#34;Tokens:-{tokens}&#34;)
print(f&#34;Token ID:-{token_ids}&#34;)
print(f&#34;ID-Token Pair:-{id_token_pair}&#34;)
Tokens:-[&#39;He&#39;, &#39;arrived&#39;, &#39;at&#39;, &#39;our&#39;, &#39;home&#39;, &#39;on&#39;, &#39;a&#39;, &#39;Sunday&#39;, &#39;of&#39;, &#39;November&#39;, &#39;,&#39;, &#39;189&#39;, &#39;-.&#39;]
Token ID:-[66, 642, 29, 113, 454, 30, 10, 2524, 7, 3305, 4, 21600, 16245]
ID-Token Pair:-[&quot;(&#39;He&#39;, 66)&quot;, &quot;(&#39;arrived&#39;, 642)&quot;, &quot;(&#39;at&#39;, 29)&quot;, &quot;(&#39;our&#39;, 113)&quot;, &quot;(&#39;home&#39;, 454)&quot;, &quot;(&#39;on&#39;, 30)&quot;, &quot;(&#39;a&#39;, 10)&quot;, &quot;(&#39;Sunday&#39;, 2524)&quot;, &quot;(&#39;of&#39;, 7)&quot;, &quot;(&#39;November&#39;, 3305)&quot;, &quot;(&#39;,&#39;, 4)&quot;, &quot;(&#39;189&#39;, 21600)&quot;, &quot;(&#39;-.&#39;, 16245)&quot;]

Build Dataset for preparing Training Data
import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset):

  def __init__(self, src_tokenizer, tgt_tokenizer, src_lang, tgt_lang, ds, seq_len):

    self.src_tokenizer = src_tokenizer
    self.tgt_tokenizer = tgt_tokenizer
    self.src_lang = src_lang
    self.tgt_lang = tgt_lang
    self.ds = ds
    self.seq_len = seq_len
    self.sos_token = torch.tensor([src_tokenizer.token_to_id(&#34;[SOC]&#34;)],dtype=torch.int64)
    self.pad_token = torch.tensor([src_tokenizer.token_to_id(&#34;[PAD]&#34;)],dtype=torch.int64)
    self.eos_token = torch.tensor([src_tokenizer.token_to_id(&#34;[EOS]&#34;)],dtype=torch.int64)

  def __len__(self):
    return len(self.ds)

  def __getitem__(self,idx):

    sentence = self.ds[&#39;translation&#39;][idx]
    src_sentence = sentence[&#39;en&#39;]
    tgt_sentence = sentence[&#39;fr&#39;]

    src_tokens = self.src_tokenizer.encode(src_sentence).ids
    tgt_tokens = self.tgt_tokenizer.encode(tgt_sentence).ids

    src_pad_tokens = self.seq_len - len(src_tokens) - 2
    tgt_pad_tokens = self.seq_len - len(tgt_tokens) - 1

    enc_input =  torch.cat([ self.sos_token,
                             torch.tensor(src_tokens,dtype=torch.int64),
                             self.eos_token,
                             torch.tensor([self.pad_token]*src_pad_tokens,dtype=torch.int64)
                             ],dim=0)

    dec_input = torch.cat([self.sos_token,
                           torch.tensor(tgt_tokens,dtype=torch.int64),
                           torch.tensor([self.pad_token]*tgt_pad_tokens,dtype=torch.int64)
                           ],dim=0)

    label = torch.cat([ tgt_pad_tokens,
                       self.eos_token,
                       torch.tensor([self.pad_token]*tgt_pad_tokens,dtype=torch.int64)
                      ],dim=0)

    return {

            &#34;encoder_input&#34; :  enc_input,
            &#34;decoder_input&#34; : dec_input,
            &#34;label&#34; : label,
            &#34;encoder_mask&#34;: (enc_input!=self.pad_token).int(),
            &#34;decoder_mask&#34;: (dec_input!=self.pad_token).int() &amp; casual_mask(dec_input.size(0)),
            &#34;src_sentence&#34;:  src_sentence,
            &#34;target_sentence&#34;: tgt_sentence

    }

  def casual_mask(size):
    mask = torch.tiru(torch.ones(1,size,size),diagonal=1,dtype=torch.int64)
    return mask==0
How Masking Works
seq_len = 20
example_src_sentence = raw_ds[&#39;translation&#39;][5][&#39;en&#39;]
example_tgt_sentence = raw_ds[&#39;translation&#39;][5][&#39;fr&#39;]
sos_token = torch.tensor([src_tokenizer.token_to_id(&#34;[SOS]&#34;)],dtype=torch.int64)
eos_token= torch.tensor([src_tokenizer.token_to_id(&#34;[EOS]&#34;)],dtype=torch.int64)
pad_token = torch.tensor([src_tokenizer.token_to_id(&#34;[PAD]&#34;)],dtype=torch.int64)
enc_input_tokens = src_tokenizer.encode(example_src_sentence).ids
dec_input_tokens = tgt_tokenizer.encode(example_tgt_sentence).ids
num_enc_pad_tokens = seq_len - len(enc_input_tokens) - 2
num_enc_pad_tokens = seq_len - len(dec_input_tokens) - 1

enc_input = torch.cat([sos_token,
           torch.tensor(enc_input_tokens,dtype=torch.int64),
           eos_token,
           torch.tensor([pad_token]* num_enc_pad_tokens,dtype=torch.int64)
           ],dim=0)

dec_input = torch.cat([sos_token,
                       torch.tensor(dec_input_tokens,dtype=torch.int64),
                       torch.tensor([pad_token]* num_enc_pad_tokens,dtype=torch.int64)
                       ],dim=0
                       )
Encoder Masking
enc_input
tensor([    1,    66,   642,    29,   113,   454,    30,    10,  2524,     7,
         3305,     4, 21600, 16245,     2,     3,     3,     3,     3,     3,
            3,     3,     3,     3])

enc_input.size(0)
24

mask = (enc_input!=pad_token).int()
mask
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       dtype=torch.int32)

enc_input.masked_fill_(mask==0,-1e9)
enc_input
tensor([          1,          66,         642,          29,         113,
                454,          30,          10,        2524,           7,
               3305,           4,       21600,       16245,           2,
        -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
        -1000000000, -1000000000, -1000000000, -1000000000])

Decoder Masking
dec_input
tensor([   1,   56,  775,  209,   52,   17, 2397,    7, 3369,    0,    0,    3,
           3,    3,    3,    3,    3,    3,    3,    3])

casual_mask = torch.triu(torch.ones((1,20,20)), diagonal=1).type(torch.int)
casual_mask==0
tensor([[[ True, False, False, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True, False, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True,  True, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])

casual_mask.shape
torch.Size([1, 20, 20])

dec_input
tensor([   1,   56,  775,  209,   52,   17, 2397,    7, 3369,    0,    0,    3,
           3,    3,    3,    3,    3,    3,    3,    3])

dec_input!=pad_token
tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True, False, False, False, False, False, False, False, False, False])

casual_mask = torch.triu(torch.ones((1,20,20)),diagonal=1).type(torch.int)
casual_mask
tensor([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],
       dtype=torch.int32)

casual_mask==0
tensor([[[ True, False, False, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True, False, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True,  True, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])

dec_mask = (dec_input!=pad_token)&amp; (casual_mask==0).int()
dec_input
tensor([   1,   56,  775,  209,   52,   17, 2397,    7, 3369,    0,    0,    3,
           3,    3,    3,    3,    3,    3,    3,    3])

dec_input.shape
torch.Size([20])

dec_input.masked_fill(dec_mask==0,-1e9)
tensor([[[          1, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000]]])
">
<meta name="author" content="Varada V N A Santosh">
<link rel="canonical" href="http://localhost:1313/posts/prepare_dataset/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/prepare_dataset/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My ML Portfolio (Alt + H)">My ML Portfolio</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Transfomer">
                    <span>Transfomer</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/posts/">Transformer Models</a></div>
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">Varada V N A Santosh

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#import-libraries" aria-label="Import Libraries">Import Libraries</a></li>
                <li>
                    <a href="#define-generator-for-training-tokenizer" aria-label="Define Generator for Training Tokenizer">Define Generator for Training Tokenizer</a></li>
                <li>
                    <a href="#build-tokenizer" aria-label="Build Tokenizer">Build Tokenizer</a></li>
                <li>
                    <a href="#train-the-tokenizer-on-dataset" aria-label="Train the Tokenizer on Dataset">Train the Tokenizer on Dataset</a></li>
                <li>
                    <a href="#understand-the-tokenizer-behaviour" aria-label="Understand the Tokenizer Behaviour">Understand the Tokenizer Behaviour</a></li>
                <li>
                    <a href="#build-dataset-for-preparing-training-data" aria-label="Build Dataset for preparing Training Data">Build Dataset for preparing Training Data</a></li>
                <li>
                    <a href="#how-masking-works" aria-label="How Masking Works">How Masking Works</a></li>
                <li>
                    <a href="#encoder-masking" aria-label="Encoder Masking">Encoder Masking</a></li>
                <li>
                    <a href="#decoder-masking" aria-label="Decoder Masking">Decoder Masking</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>pip install datasets
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>pip install huggingface_hub
</span></span></code></pre></div><pre><code>Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)
Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)
Requirement already satisfied: pyarrow&gt;=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)
Requirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)
Requirement already satisfied: requests&gt;=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)
Requirement already satisfied: tqdm&gt;=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)
Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess&lt;0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec&lt;=2024.12.0,&gt;=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]&lt;=2024.12.0,&gt;=2023.1.0-&gt;datasets) (2024.12.0)
Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)
Requirement already satisfied: huggingface-hub&gt;=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)
Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)
Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (2.6.1)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.3.2)
Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (25.3.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.5.0)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (6.2.0)
Requirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (0.3.0)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.18.3)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub&gt;=0.24.0-&gt;datasets) (4.12.2)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.4.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2.3.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2025.1.31)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2.8.2)
Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.1)
Requirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.1)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.17.0)
Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)
Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)
Requirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)
Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)
Requirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;huggingface_hub) (3.4.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;huggingface_hub) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;huggingface_hub) (2.3.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;huggingface_hub) (2025.1.31)
</code></pre>
<h1 id="import-libraries">Import Libraries<a hidden class="anchor" aria-hidden="true" href="#import-libraries">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tokenizers <span style="color:#f92672">import</span> Tokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tokenizers.models <span style="color:#f92672">import</span> WordLevel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tokenizers.trainers <span style="color:#f92672">import</span> WordLevelTrainer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tokenizers.pre_tokenizers <span style="color:#f92672">import</span> Whitespace
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> load_dataset
</span></span></code></pre></div><h1 id="define-generator-for-training-tokenizer">Define Generator for Training Tokenizer<a hidden class="anchor" aria-hidden="true" href="#define-generator-for-training-tokenizer">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_ds</span>(ds,lang):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> sentence <span style="color:#f92672">in</span> ds[<span style="color:#e6db74">&#39;translation&#39;</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">yield</span> sentence[lang]
</span></span></code></pre></div><h1 id="build-tokenizer">Build Tokenizer<a hidden class="anchor" aria-hidden="true" href="#build-tokenizer">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_tokenizer</span>(ds,lang):
</span></span><span style="display:flex;"><span>  tokenizer <span style="color:#f92672">=</span> Tokenizer(WordLevel(unk_token<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;[UNK]&#34;</span>))
</span></span><span style="display:flex;"><span>  tokenizer<span style="color:#f92672">.</span>pre_tokenizer <span style="color:#f92672">=</span> Whitespace()
</span></span><span style="display:flex;"><span>  trainer <span style="color:#f92672">=</span> WordLevelTrainer(special_tokens<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;[UNK]&#34;</span>,<span style="color:#e6db74">&#34;[SOS]&#34;</span>,<span style="color:#e6db74">&#34;[EOS]&#34;</span>,<span style="color:#e6db74">&#34;[PAD]&#34;</span>],min_frequency<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>  tokenizer<span style="color:#f92672">.</span>train_from_iterator(get_ds(ds,lang),trainer)
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> tokenizer
</span></span></code></pre></div><h1 id="train-the-tokenizer-on-dataset">Train the Tokenizer on Dataset<a hidden class="anchor" aria-hidden="true" href="#train-the-tokenizer-on-dataset">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>raw_ds <span style="color:#f92672">=</span> load_dataset(<span style="color:#e6db74">&#39;opus_books&#39;</span>,<span style="color:#e6db74">&#39;en-fr&#39;</span>,split<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train&#39;</span>)
</span></span><span style="display:flex;"><span>src_tokenizer <span style="color:#f92672">=</span> build_tokenizer(raw_ds,<span style="color:#e6db74">&#39;en&#39;</span>)
</span></span><span style="display:flex;"><span>tgt_tokenizer <span style="color:#f92672">=</span> build_tokenizer(raw_ds,<span style="color:#e6db74">&#39;fr&#39;</span>)
</span></span></code></pre></div><h1 id="understand-the-tokenizer-behaviour">Understand the Tokenizer Behaviour<a hidden class="anchor" aria-hidden="true" href="#understand-the-tokenizer-behaviour">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>src_tokenizer<span style="color:#f92672">.</span>get_vocab_size()
</span></span></code></pre></div><pre><code>30000
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> OrderedDict
</span></span><span style="display:flex;"><span>vocab <span style="color:#f92672">=</span> src_tokenizer<span style="color:#f92672">.</span>get_vocab()
</span></span><span style="display:flex;"><span>sorted_vocab <span style="color:#f92672">=</span> sorted(vocab<span style="color:#f92672">.</span>items(),key <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x:x[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>ordered_vocab<span style="color:#f92672">=</span>OrderedDict(sorted_vocab)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>example_sentence <span style="color:#f92672">=</span> raw_ds[<span style="color:#e6db74">&#39;translation&#39;</span>][<span style="color:#ae81ff">5</span>][<span style="color:#e6db74">&#39;en&#39;</span>]
</span></span><span style="display:flex;"><span>tokens<span style="color:#f92672">=</span> src_tokenizer<span style="color:#f92672">.</span>encode(example_sentence)<span style="color:#f92672">.</span>tokens
</span></span><span style="display:flex;"><span>token_ids <span style="color:#f92672">=</span> src_tokenizer<span style="color:#f92672">.</span>encode(example_sentence)<span style="color:#f92672">.</span>ids
</span></span><span style="display:flex;"><span>id_token_pair <span style="color:#f92672">=</span> [<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>(token,ordered_vocab[token])<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Tokens:-</span><span style="color:#e6db74">{</span>tokens<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Token ID:-</span><span style="color:#e6db74">{</span>token_ids<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;ID-Token Pair:-</span><span style="color:#e6db74">{</span>id_token_pair<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>Tokens:-['He', 'arrived', 'at', 'our', 'home', 'on', 'a', 'Sunday', 'of', 'November', ',', '189', '-.']
Token ID:-[66, 642, 29, 113, 454, 30, 10, 2524, 7, 3305, 4, 21600, 16245]
ID-Token Pair:-[&quot;('He', 66)&quot;, &quot;('arrived', 642)&quot;, &quot;('at', 29)&quot;, &quot;('our', 113)&quot;, &quot;('home', 454)&quot;, &quot;('on', 30)&quot;, &quot;('a', 10)&quot;, &quot;('Sunday', 2524)&quot;, &quot;('of', 7)&quot;, &quot;('November', 3305)&quot;, &quot;(',', 4)&quot;, &quot;('189', 21600)&quot;, &quot;('-.', 16245)&quot;]
</code></pre>
<h1 id="build-dataset-for-preparing-training-data">Build Dataset for preparing Training Data<a hidden class="anchor" aria-hidden="true" href="#build-dataset-for-preparing-training-data">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> Dataset
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CustomDataset</span>(Dataset):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> __init__(self, src_tokenizer, tgt_tokenizer, src_lang, tgt_lang, ds, seq_len):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>src_tokenizer <span style="color:#f92672">=</span> src_tokenizer
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>tgt_tokenizer <span style="color:#f92672">=</span> tgt_tokenizer
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>src_lang <span style="color:#f92672">=</span> src_lang
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>tgt_lang <span style="color:#f92672">=</span> tgt_lang
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>ds <span style="color:#f92672">=</span> ds
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>seq_len <span style="color:#f92672">=</span> seq_len
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>sos_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([src_tokenizer<span style="color:#f92672">.</span>token_to_id(<span style="color:#e6db74">&#34;[SOC]&#34;</span>)],dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>pad_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([src_tokenizer<span style="color:#f92672">.</span>token_to_id(<span style="color:#e6db74">&#34;[PAD]&#34;</span>)],dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>eos_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([src_tokenizer<span style="color:#f92672">.</span>token_to_id(<span style="color:#e6db74">&#34;[EOS]&#34;</span>)],dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> __len__(self):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>ds)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> __getitem__(self,idx):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sentence <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ds[<span style="color:#e6db74">&#39;translation&#39;</span>][idx]
</span></span><span style="display:flex;"><span>    src_sentence <span style="color:#f92672">=</span> sentence[<span style="color:#e6db74">&#39;en&#39;</span>]
</span></span><span style="display:flex;"><span>    tgt_sentence <span style="color:#f92672">=</span> sentence[<span style="color:#e6db74">&#39;fr&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    src_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>src_tokenizer<span style="color:#f92672">.</span>encode(src_sentence)<span style="color:#f92672">.</span>ids
</span></span><span style="display:flex;"><span>    tgt_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>tgt_tokenizer<span style="color:#f92672">.</span>encode(tgt_sentence)<span style="color:#f92672">.</span>ids
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    src_pad_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>seq_len <span style="color:#f92672">-</span> len(src_tokens) <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>    tgt_pad_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>seq_len <span style="color:#f92672">-</span> len(tgt_tokens) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    enc_input <span style="color:#f92672">=</span>  torch<span style="color:#f92672">.</span>cat([ self<span style="color:#f92672">.</span>sos_token,
</span></span><span style="display:flex;"><span>                             torch<span style="color:#f92672">.</span>tensor(src_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64),
</span></span><span style="display:flex;"><span>                             self<span style="color:#f92672">.</span>eos_token,
</span></span><span style="display:flex;"><span>                             torch<span style="color:#f92672">.</span>tensor([self<span style="color:#f92672">.</span>pad_token]<span style="color:#f92672">*</span>src_pad_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>                             ],dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    dec_input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([self<span style="color:#f92672">.</span>sos_token,
</span></span><span style="display:flex;"><span>                           torch<span style="color:#f92672">.</span>tensor(tgt_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64),
</span></span><span style="display:flex;"><span>                           torch<span style="color:#f92672">.</span>tensor([self<span style="color:#f92672">.</span>pad_token]<span style="color:#f92672">*</span>tgt_pad_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>                           ],dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    label <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([ tgt_pad_tokens,
</span></span><span style="display:flex;"><span>                       self<span style="color:#f92672">.</span>eos_token,
</span></span><span style="display:flex;"><span>                       torch<span style="color:#f92672">.</span>tensor([self<span style="color:#f92672">.</span>pad_token]<span style="color:#f92672">*</span>tgt_pad_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>                      ],dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;encoder_input&#34;</span> :  enc_input,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;decoder_input&#34;</span> : dec_input,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;label&#34;</span> : label,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;encoder_mask&#34;</span>: (enc_input<span style="color:#f92672">!=</span>self<span style="color:#f92672">.</span>pad_token)<span style="color:#f92672">.</span>int(),
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;decoder_mask&#34;</span>: (dec_input<span style="color:#f92672">!=</span>self<span style="color:#f92672">.</span>pad_token)<span style="color:#f92672">.</span>int() <span style="color:#f92672">&amp;</span> casual_mask(dec_input<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)),
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;src_sentence&#34;</span>:  src_sentence,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;target_sentence&#34;</span>: tgt_sentence
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">casual_mask</span>(size):
</span></span><span style="display:flex;"><span>    mask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tiru(torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">1</span>,size,size),diagonal<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> mask<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>
</span></span></code></pre></div><h1 id="how-masking-works">How Masking Works<a hidden class="anchor" aria-hidden="true" href="#how-masking-works">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>seq_len <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>example_src_sentence <span style="color:#f92672">=</span> raw_ds[<span style="color:#e6db74">&#39;translation&#39;</span>][<span style="color:#ae81ff">5</span>][<span style="color:#e6db74">&#39;en&#39;</span>]
</span></span><span style="display:flex;"><span>example_tgt_sentence <span style="color:#f92672">=</span> raw_ds[<span style="color:#e6db74">&#39;translation&#39;</span>][<span style="color:#ae81ff">5</span>][<span style="color:#e6db74">&#39;fr&#39;</span>]
</span></span><span style="display:flex;"><span>sos_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([src_tokenizer<span style="color:#f92672">.</span>token_to_id(<span style="color:#e6db74">&#34;[SOS]&#34;</span>)],dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>eos_token<span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([src_tokenizer<span style="color:#f92672">.</span>token_to_id(<span style="color:#e6db74">&#34;[EOS]&#34;</span>)],dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>pad_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([src_tokenizer<span style="color:#f92672">.</span>token_to_id(<span style="color:#e6db74">&#34;[PAD]&#34;</span>)],dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>enc_input_tokens <span style="color:#f92672">=</span> src_tokenizer<span style="color:#f92672">.</span>encode(example_src_sentence)<span style="color:#f92672">.</span>ids
</span></span><span style="display:flex;"><span>dec_input_tokens <span style="color:#f92672">=</span> tgt_tokenizer<span style="color:#f92672">.</span>encode(example_tgt_sentence)<span style="color:#f92672">.</span>ids
</span></span><span style="display:flex;"><span>num_enc_pad_tokens <span style="color:#f92672">=</span> seq_len <span style="color:#f92672">-</span> len(enc_input_tokens) <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>num_enc_pad_tokens <span style="color:#f92672">=</span> seq_len <span style="color:#f92672">-</span> len(dec_input_tokens) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>enc_input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sos_token,
</span></span><span style="display:flex;"><span>           torch<span style="color:#f92672">.</span>tensor(enc_input_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64),
</span></span><span style="display:flex;"><span>           eos_token,
</span></span><span style="display:flex;"><span>           torch<span style="color:#f92672">.</span>tensor([pad_token]<span style="color:#f92672">*</span> num_enc_pad_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>           ],dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dec_input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([sos_token,
</span></span><span style="display:flex;"><span>                       torch<span style="color:#f92672">.</span>tensor(dec_input_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64),
</span></span><span style="display:flex;"><span>                       torch<span style="color:#f92672">.</span>tensor([pad_token]<span style="color:#f92672">*</span> num_enc_pad_tokens,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int64)
</span></span><span style="display:flex;"><span>                       ],dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>                       )
</span></span></code></pre></div><h1 id="encoder-masking">Encoder Masking<a hidden class="anchor" aria-hidden="true" href="#encoder-masking">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>enc_input
</span></span></code></pre></div><pre><code>tensor([    1,    66,   642,    29,   113,   454,    30,    10,  2524,     7,
         3305,     4, 21600, 16245,     2,     3,     3,     3,     3,     3,
            3,     3,     3,     3])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>enc_input<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><pre><code>24
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mask <span style="color:#f92672">=</span> (enc_input<span style="color:#f92672">!=</span>pad_token)<span style="color:#f92672">.</span>int()
</span></span><span style="display:flex;"><span>mask
</span></span></code></pre></div><pre><code>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       dtype=torch.int32)
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>enc_input<span style="color:#f92672">.</span>masked_fill_(mask<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>)
</span></span><span style="display:flex;"><span>enc_input
</span></span></code></pre></div><pre><code>tensor([          1,          66,         642,          29,         113,
                454,          30,          10,        2524,           7,
               3305,           4,       21600,       16245,           2,
        -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
        -1000000000, -1000000000, -1000000000, -1000000000])
</code></pre>
<h1 id="decoder-masking">Decoder Masking<a hidden class="anchor" aria-hidden="true" href="#decoder-masking">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dec_input
</span></span></code></pre></div><pre><code>tensor([   1,   56,  775,  209,   52,   17, 2397,    7, 3369,    0,    0,    3,
           3,    3,    3,    3,    3,    3,    3,    3])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>casual_mask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>triu(torch<span style="color:#f92672">.</span>ones((<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">20</span>,<span style="color:#ae81ff">20</span>)), diagonal<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>type(torch<span style="color:#f92672">.</span>int)
</span></span><span style="display:flex;"><span>casual_mask<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>
</span></span></code></pre></div><pre><code>tensor([[[ True, False, False, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True, False, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True,  True, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>casual_mask<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><pre><code>torch.Size([1, 20, 20])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dec_input
</span></span></code></pre></div><pre><code>tensor([   1,   56,  775,  209,   52,   17, 2397,    7, 3369,    0,    0,    3,
           3,    3,    3,    3,    3,    3,    3,    3])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dec_input<span style="color:#f92672">!=</span>pad_token
</span></span></code></pre></div><pre><code>tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True, False, False, False, False, False, False, False, False, False])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>casual_mask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>triu(torch<span style="color:#f92672">.</span>ones((<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">20</span>,<span style="color:#ae81ff">20</span>)),diagonal<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>type(torch<span style="color:#f92672">.</span>int)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>casual_mask
</span></span></code></pre></div><pre><code>tensor([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],
       dtype=torch.int32)
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>casual_mask<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>
</span></span></code></pre></div><pre><code>tensor([[[ True, False, False, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True, False, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True, False, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True, False, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True, False, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True, False, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True, False, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True, False, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          False, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True, False, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True, False, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True, False, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True, False, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True, False, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True, False, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True, False, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True, False, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True,  True, False],
         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
           True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dec_mask <span style="color:#f92672">=</span> (dec_input<span style="color:#f92672">!=</span>pad_token)<span style="color:#f92672">&amp;</span> (casual_mask<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>int()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dec_input
</span></span></code></pre></div><pre><code>tensor([   1,   56,  775,  209,   52,   17, 2397,    7, 3369,    0,    0,    3,
           3,    3,    3,    3,    3,    3,    3,    3])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dec_input<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><pre><code>torch.Size([20])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dec_input<span style="color:#f92672">.</span>masked_fill(dec_mask<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>)
</span></span></code></pre></div><pre><code>tensor([[[          1, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000],
         [          1,          56,         775,         209,          52,
                   17,        2397,           7,        3369,           0,
                    0, -1000000000, -1000000000, -1000000000, -1000000000,
          -1000000000, -1000000000, -1000000000, -1000000000, -1000000000]]])
</code></pre>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">My ML Portfolio</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
