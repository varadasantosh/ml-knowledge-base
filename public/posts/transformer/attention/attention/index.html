<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Evolution of Attention | </title>
<meta name="keywords" content="">
<meta name="description" content="Evolution of Attention - ">
<meta name="author" content="Varada V N A Santosh">
<link rel="canonical" href="http://localhost:1313/posts/transformer/attention/attention/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/transformer/attention/attention/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

>
<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/transformer/">Transformer Models</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/transformer/attention/">Attention Mechanisms</a></div>
    <h1 class="post-title entry-hint-parent">
      Evolution of Attention
    </h1>
    <div class="post-meta">Varada V N A Santosh

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#evolution-of-attention-mechanism" aria-label="Evolution of Attention Mechanism">Evolution of Attention Mechanism</a></li>
                <li>
                    <a href="#attention-in-transformers" aria-label="Attention in Transformers">Attention in Transformers</a></li>
                <li>
                    <a href="#implementation" aria-label="Implementation">Implementation</a></li>
                <li>
                    <a href="#visualizing-self-attention-using-llama-model" aria-label="Visualizing Self Attention using Llama Model">Visualizing Self Attention using Llama Model</a><ul>
                        
                <li>
                    <a href="#llama-model-architecture" aria-label="Llama Model Architecture">Llama Model Architecture</a></li>
                <li>
                    <a href="#tokenize-the-input-sentence--pass-it-through-the-llama-model" aria-label="Tokenize the Input Sentence &amp; Pass it through the Llama Model">Tokenize the Input Sentence &amp; Pass it through the Llama Model</a></li>
                <li>
                    <a href="#observe-the-attention-scores" aria-label="Observe the Attention Scores">Observe the Attention Scores</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="evolution-of-attention-mechanism">Evolution of Attention Mechanism<a hidden class="anchor" aria-hidden="true" href="#evolution-of-attention-mechanism">#</a></h1>
<p>Attention was firts introduced as part of seq-to-seq (Encoder-Decoder) models in the domain of Neural Machine Translation to translate text from one language to other language. Initial Architectures of encoder-decoder models were composed of encoder and decoder both are RNN&rsquo;s, it is also possible to combine both simple RNN as part of encoder and GRU or LSTM for decoder , encoder takes sentence in source language as input and generates context vector of fixed lentgh , which would be passed as input to Decoder , Decoder takes the context vector and tries to map it to corresponding word or text in target language, this has few limitations as the single context vector generated by the encoder RNN could not capture the entire meaning of the sentence in source language which resulted less accurate results, especially as the length of the sequence grows the accuracy drops.</p>
<p>Inorder to overcome the issues of initial seq-to-seq models, researchers came up with an approach to capture all hidden states of encoder pass them to decoder to capture the meaning or context , but now the challenge is to know which hidden state could be contributing more to find the next word in target language, this is not simple as the source and target languages have different semantics , researchers came up with an approach to build alignment model (Single Layer Feed Forward Neural Network) that takes hidden state of previous decoder timestep $S_{i-
1}$ and encoder hidden state $h_{j}$ vector to build context vector $C_{t}$ using alignment model , the alignment model computes the compatability scores between the previous decoder hidden state and each hidden state of encoder , thus computed compatability scores are passed through softmax function to normalize the scores, these scores are multiplied with each hidden state
of the encoder to calculate the weighted scores of the encoder hidden states, all these weighted hidden states are added which results in context vector this is passed as one of the inputs the Decoder timestep $S_{i}$ along with hidden state of previous decoder timestep, <strong>this lays the foundation for the Attention Mechanism, the attention that we discussed is Bahdanau Attention
this is also called Additive attention as we are adding all the context vectors to calculate the alignment scores</strong>, this triggered further improvements and <code>Loung Attention</code> proposed different ways to calculate alginment scores to calculate the relevance between each hiddent vector of encoder and current decoder state, as part of Loung attention they also managed to avoid
the alginment model, which reduces the number of parameters to be trained. Below is the reference picture of how Bahdanu Attention works</p>
<p><img alt="image" loading="lazy" src="https://github.com/user-attachments/assets/ddd1c4a0-165c-4fc7-a984-d24ea680cb90"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"></code></pre></div><h1 id="attention-in-transformers">Attention in Transformers<a hidden class="anchor" aria-hidden="true" href="#attention-in-transformers">#</a></h1>
<p>The above mentioned attentions <strong>Bahdanau &amp; Luong</strong>  paved way for attention in Transformers, there are few disadvantages with the prior Attention mechanism major one being both of them are
sequential in nature, as we process one token after the other this makes training process tedious and time taking, as we see the birth the of Large Language models that are trained on
Billions of tokens, this would not have been possible without Self Attention which calculates these Attention scores in parallel which were referred as Alignment scores in Bahdanau &amp; Loung
Attentions, to make this parallel processing possible Self Attention follows below steps.</p>
<ol>
<li>
<p>Tokenize Sentence - Breaks the sentence into tokens</p>
</li>
<li>
<p>Generate Embeddings for the tokens</p>
</li>
<li>
<p>Pass the Embeddings tokens through different Linear Layers to generate Q,K,V &amp; O matrices, each linear layer has its corresponding weight matrices, $W_{Q}$ , $W_{K}$ $W_{V}$ &amp; $W_{O}$
these weights are learned through the training process.</p>
<ul>
<li>X * $W_{Q}$ = Q - Query Vector</li>
<li>X * $W_{K}$ = K - Key Vector</li>
<li>X * $W_{V}$ = V - Value Vector</li>
<li>X * $W_{O}$ = O - Output Vector</li>
</ul>
</li>
</ol>
<pre><code>**Dimensions**

- X -&gt; T  * $d_{model}$
- $W_{Q}$ -&gt; $d_{model}$ * $d_{k}$
- $W_{K}$ -&gt; $d_{model}$ * $d_{k}$
- $W_{V}$ -&gt; $d_{model}$ * $d_{k}$
- $W_{O}$ -&gt; $d_{model}$ * $d_{k}$

- T - Sequence Length
- $d_{model}$  - Length of Embeddings
- $d_{k}$ - Output dimensions of $W_{Q}$,$W_{K}$ &amp; $W_{V}$, this can be
  same as $d_{model}$ as well
</code></pre>
<ol start="4">
<li>
<p>Calculate the Scaled Dot Product Between Q (Query) &amp; K (Key) vectors to
find how each token relates to other token,this is simialr to calculation of alignment scores in earlier Seq-to-Seq RNN models</p>
<p><strong>Scaled Dot Product Attention</strong>: -   $\left( \frac{QK^T}{\sqrt{d_k}} \right)$</p>
<p><img alt="image" loading="lazy" src="https://github.com/user-attachments/assets/25cd0ce2-7734-4ecf-b0f3-e33cce29cd78"></p>
</li>
<li>
<p>Result of Scaled Dot Product Attention is passed through Softmax to
normalize the attention scores</p>
<p><strong>Normalize Attention Scores</strong>:-  $\text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)$</p>
</li>
<li>
<p>Multiply these Attention scores with $W_{V}$ to calculate the weighted attentions</p>
</li>
<li>
<p>Result of the Weighted attentions is thus multiplied by $W_{O}$ output projections.</p>
<p>Below is the code snippet that explains above steps briefly, though this is not exactly what is being used in Transformer Architecture, as we use Multi Head Attention which we will discuss
but this is the core of the  Attention calculation</p>
</li>
</ol>
<h1 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h1>
<p>Below is the code snippet that explains above steps briefly, most of Transformer Architectures use Multi Head Attention to calcualte the attention score, below gives an idea on how attention scores are calculated</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch <span style="color:#f92672">import</span> Tensor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Attention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, embedding_dim: int, attention_dim: int):
</span></span><span style="display:flex;"><span>    super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Initialising weights</span>
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>wk <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embedding_dim, attention_dim, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>wq <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embedding_dim, attention_dim, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>wv <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(embedding_dim, attention_dim, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, embedded: Tensor) <span style="color:#f92672">-&gt;</span> Tensor:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># calculating Query, Key and Value</span>
</span></span><span style="display:flex;"><span>    q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>wq(embedded)
</span></span><span style="display:flex;"><span>    k <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>wk(embedded)
</span></span><span style="display:flex;"><span>    v <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>wv(embedded)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># calculating attention scores</span>
</span></span><span style="display:flex;"><span>    attn_score <span style="color:#f92672">=</span> q <span style="color:#f92672">@</span> torch<span style="color:#f92672">.</span>transpose(k, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> (k<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">**</span> <span style="color:#ae81ff">0.5</span>) <span style="color:#75715e"># [batch_size, num_words, num_words]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># below 2 lines is for masking in decoder block</span>
</span></span><span style="display:flex;"><span>    upper_triangular  <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>triu(attn_score, diagonal<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>bool()
</span></span><span style="display:flex;"><span>    attn_score[upper_triangular] <span style="color:#f92672">=</span> float(<span style="color:#e6db74">&#34;-inf&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># applying softmax</span>
</span></span><span style="display:flex;"><span>    attn_score_softmax <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>softmax(attn_score, dim <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># [batch_size, num_words, num_words]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># getting weighted values by multiplying softmax of attention score with values</span>
</span></span><span style="display:flex;"><span>    weighted_values <span style="color:#f92672">=</span> attn_score_softmax <span style="color:#f92672">@</span> v <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> weighted_values
</span></span></code></pre></div><h1 id="visualizing-self-attention-using-llama-model">Visualizing Self Attention using Llama Model<a hidden class="anchor" aria-hidden="true" href="#visualizing-self-attention-using-llama-model">#</a></h1>
<p>##Download the Llama Model from Hugging Face</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModel, AutoModelForCausalLM
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_name<span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;meta-llama/Llama-3.2-3B-Instruct&#34;</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(model_name, output_attentions<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><pre><code>tokenizer_config.json:   0%|          | 0.00/54.5k [00:00&lt;?, ?B/s]



tokenizer.json:   0%|          | 0.00/9.09M [00:00&lt;?, ?B/s]



special_tokens_map.json:   0%|          | 0.00/296 [00:00&lt;?, ?B/s]



config.json:   0%|          | 0.00/878 [00:00&lt;?, ?B/s]



model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00&lt;?, ?B/s]



Fetching 2 files:   0%|          | 0/2 [00:00&lt;?, ?it/s]



model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00&lt;?, ?B/s]



model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00&lt;?, ?B/s]



Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]
</code></pre>
<h2 id="llama-model-architecture">Llama Model Architecture<a hidden class="anchor" aria-hidden="true" href="#llama-model-architecture">#</a></h2>
<p>(Llama is Decoder only Transformer , hence we can see there are only 28 Decoder Layers, zero encoder layers)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(model)
</span></span></code></pre></div><pre><code>LlamaModel(
  (embed_tokens): Embedding(128256, 3072)
  (layers): ModuleList(
    (0-27): 28 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=3072, out_features=3072, bias=False)
        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
        (v_proj): Linear(in_features=3072, out_features=1024, bias=False)
        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
        (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
    )
  )
  (norm): LlamaRMSNorm((3072,), eps=1e-05)
  (rotary_emb): LlamaRotaryEmbedding()
)
</code></pre>
<h2 id="tokenize-the-input-sentence--pass-it-through-the-llama-model">Tokenize the Input Sentence &amp; Pass it through the Llama Model<a hidden class="anchor" aria-hidden="true" href="#tokenize-the-input-sentence--pass-it-through-the-llama-model">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;the financial bank is located on river bank&#34;</span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer(text, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span>token_ids <span style="color:#f92672">=</span> inputs<span style="color:#f92672">.</span>input_ids[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>convert_ids_to_tokens(token_ids)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> inputs<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span>    outputs <span style="color:#f92672">=</span> model(<span style="color:#f92672">**</span>inputs)
</span></span></code></pre></div><pre><code>`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=&quot;eager&quot;` when loading the model.
</code></pre>
<p>Get The Attention Matrix from the Outputs, there are 28 Layers , we can see the below dimensions of the <code>attention_matrix</code> of length 28 &amp; each layer&rsquo;s attention matrix is of shape (1,24,9,9) - This is because Llama Model has 24 Heads (This refers to Multi Head attention) and sequence length of tokens that we passed is of length 9 hence the dimension of each head is 9*9</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attention_matrix <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>attentions
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Number of Attention Matrices == Number of Layers:  </span><span style="color:#e6db74">{</span>len(attention_matrix)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Shape of Each Attention Matrix </span><span style="color:#e6db74">{</span>attention_matrix[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>Number of Attention Matrices == Number of Layers:  28
Shape of Each Attention Matrix torch.Size([1, 24, 9, 9])
</code></pre>
<h2 id="observe-the-attention-scores">Observe the Attention Scores<a hidden class="anchor" aria-hidden="true" href="#observe-the-attention-scores">#</a></h2>
<p>Get Attentions from final layer, calculate the avg attention scores across all heads and plot the heatmap to find relation ship, though from the below heatmap we can&rsquo;t find stronger
contextual relation ship between tokens like financial &amp; bank , river &amp; bank we can see them when we go through individual heads of multihead attention, but one thing we can observe in
the attention score heatmap is all the elements above diagonal are zero. This is because the Decoder part of model has casual attention which prevents each token from attending to future        tokens of the sequence, this is important as transformers do the self attention in parallel, where as in RNN the attention always sequentially , hence we don&rsquo;t step on to future
tokens, in transformers this is not the case as we are processing all the tokens in parallel.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>avg_attn <span style="color:#f92672">=</span>attention_matrix[<span style="color:#ae81ff">27</span>][<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>heatmap(avg_attn<span style="color:#f92672">.</span>cpu(), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;viridis&#34;</span>,annot<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,fmt<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;.2f&#34;</span>,xticklabels<span style="color:#f92672">=</span>tokens,yticklabels<span style="color:#f92672">=</span>tokens )
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Attention Matrix (Layer 28)&#34;</span>,fontdict<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;fontsize&#39;</span>:<span style="color:#ae81ff">25</span>})
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img alt="Visualization" loading="lazy" src="/images/transformers/Attentions/self_attention.png"></p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/"></a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
