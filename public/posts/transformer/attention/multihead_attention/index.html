<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Peek Into MultiHead Attention | </title>
<meta name="keywords" content="">
<meta name="description" content="Peek Into MultiHead Attention - ">
<meta name="author" content="Varada V N A Santosh">
<link rel="canonical" href="http://localhost:1313/posts/transformer/attention/multihead_attention/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/transformer/attention/multihead_attention/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

>
<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/transformer/">Transformer Models</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/transformer/attention/">Attention Mechanisms</a></div>
    <h1 class="post-title entry-hint-parent">
      Peek Into MultiHead Attention
    </h1>
    <div class="post-meta">Varada V N A Santosh

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#multihead-attention" aria-label="MultiHead Attention">MultiHead Attention</a></li>
                <li>
                    <a href="#reference-implementation" aria-label="Reference Implementation">Reference Implementation</a></li>
                <li>
                    <a href="#visulization-of-multihead-attention-using-llama-model" aria-label="Visulization of MultiHead Attention using Llama Model">Visulization of MultiHead Attention using Llama Model</a><ul>
                        
                <li>
                    <a href="#download-the-llama-model-from-hugging-face" aria-label="Download the Llama Model from Hugging Face">Download the Llama Model from Hugging Face</a></li>
                <li>
                    <a href="#tokenize-the-input-sentence--pass-it-through-the-llama-model" aria-label="Tokenize the Input Sentence &amp; Pass it through the Llama Model">Tokenize the Input Sentence &amp; Pass it through the Llama Model</a></li></ul>
                </li>
                <li>
                    <a href="#focus-on-one-of-attention-head" aria-label="Focus on One of Attention Head">Focus on One of Attention Head</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="multihead-attention">MultiHead Attention<a hidden class="anchor" aria-hidden="true" href="#multihead-attention">#</a></h1>
<p>The Multi Head Attention is an extension to Self Attention, while the Self Attentin defined in Transfomers helps us to overcome limitations faced by RNN&rsquo;s, if we look at the above pitcutre we are calculating the attention over all heads of Llama Model , Multi Head Attention helps us to attend diferent aspects of elements in a sequence, in such case single weighted average is not
good option, to understand different aspects we divide the Query, Key &amp; Value matrices to different Heads and calculate the attention scores of each head, to calculate attention for each head
we apply the same approach mentioned above,after the attention scores of each head are calculated we concatenate the attention scores of all the heads, this approach yeilds better results than
finding the attention as a whole, during this process the weight matrices that are split are learned for each head.</p>
<p>Llama Model(<code>Llama-3.2-3B-Instruct</code>) referred above has Embedding Dimensions of size - <strong>3072</strong> &amp;  Number of Heads - <strong>24</strong>, thus our Query , Key &amp; Values are split into 24 heads each head would be of size 3072/24 = 128</p>
<p>Multihead(Q,K,V) =  Concat($head_{1}$, $head_{2}$,&hellip;..$head_{24}$)
$head_{i}$ = Attention(Q$W_{i}$^Q, K$W_{i}$^K, V$W_{i}$^V)</p>
<p>Below image illustrates the process of calculating MultiHead Attention</p>
<p><img alt="image" loading="lazy" src="https://github.com/user-attachments/assets/67f3bb0b-1ac6-454e-ab5c-6975c368a9e3"></p>
<h1 id="reference-implementation">Reference Implementation<a hidden class="anchor" aria-hidden="true" href="#reference-implementation">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MultiHeadAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self,d_model,num_heads):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        super(MultiHeadAttention,self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_model <span style="color:#f92672">=</span> d_model
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_heads <span style="color:#f92672">=</span> num_heads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_k <span style="color:#f92672">=</span> d_model <span style="color:#f92672">//</span> num_heads
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_q<span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model,d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_k<span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model,d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_v<span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model,d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_o<span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model,d_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">scaled_dot_product_attention</span>(self,Q,K,V,mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attention_scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(Q,K<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))<span style="color:#f92672">/</span>math<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>d_k)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            attention_scores <span style="color:#f92672">=</span> attention_scores<span style="color:#f92672">.</span>masked_fill(mask<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attention_probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(attention_scores,dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(attention_probs,V)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">split_heads</span>(self,x):
</span></span><span style="display:flex;"><span>        batch_size,seq_length,d_model <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x<span style="color:#f92672">.</span>view(batch_size,seq_length,self<span style="color:#f92672">.</span>num_heads,self<span style="color:#f92672">.</span>d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">combine_heads</span>(self,x):
</span></span><span style="display:flex;"><span>        batch_size,_,seq_length,d_k <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(batch_size,seq_length,self<span style="color:#f92672">.</span>d_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self,Q,K,V,mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        Q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>split_heads(self<span style="color:#f92672">.</span>W_q(Q))
</span></span><span style="display:flex;"><span>        K <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>split_heads(self<span style="color:#f92672">.</span>W_k(K))
</span></span><span style="display:flex;"><span>        V <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>split_heads(self<span style="color:#f92672">.</span>W_v(V))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attention_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>scaled_dot_product_attention(Q,K,V,mask)
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_o(self<span style="color:#f92672">.</span>combine_heads(attention_output))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div><h1 id="visulization-of-multihead-attention-using-llama-model">Visulization of MultiHead Attention using Llama Model<a hidden class="anchor" aria-hidden="true" href="#visulization-of-multihead-attention-using-llama-model">#</a></h1>
<p>We use the same Model &amp; Input text we considered for Self Attention and look at one of the Heads of the Last layer in to see if they are able to attend contextual relation ships between different tokens</p>
<h2 id="download-the-llama-model-from-hugging-face">Download the Llama Model from Hugging Face<a hidden class="anchor" aria-hidden="true" href="#download-the-llama-model-from-hugging-face">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModel, AutoModelForCausalLM
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_name<span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;meta-llama/Llama-3.2-3B-Instruct&#34;</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(model_name, output_attentions<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><pre><code>tokenizer_config.json:   0%|          | 0.00/54.5k [00:00&lt;?, ?B/s]



tokenizer.json:   0%|          | 0.00/9.09M [00:00&lt;?, ?B/s]



special_tokens_map.json:   0%|          | 0.00/296 [00:00&lt;?, ?B/s]



config.json:   0%|          | 0.00/878 [00:00&lt;?, ?B/s]



model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00&lt;?, ?B/s]



Fetching 2 files:   0%|          | 0/2 [00:00&lt;?, ?it/s]



model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00&lt;?, ?B/s]



model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00&lt;?, ?B/s]



Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]
</code></pre>
<p>Llama Model Architecture , Llama model being Decoder only Transformer we can see there are only Decoder Layers (28 Layers), zero encoder layers</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(model)
</span></span></code></pre></div><pre><code>LlamaModel(
  (embed_tokens): Embedding(128256, 3072)
  (layers): ModuleList(
    (0-27): 28 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=3072, out_features=3072, bias=False)
        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)
        (v_proj): Linear(in_features=3072, out_features=1024, bias=False)
        (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)
        (up_proj): Linear(in_features=3072, out_features=8192, bias=False)
        (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)
    )
  )
  (norm): LlamaRMSNorm((3072,), eps=1e-05)
  (rotary_emb): LlamaRotaryEmbedding()
)
</code></pre>
<h2 id="tokenize-the-input-sentence--pass-it-through-the-llama-model">Tokenize the Input Sentence &amp; Pass it through the Llama Model<a hidden class="anchor" aria-hidden="true" href="#tokenize-the-input-sentence--pass-it-through-the-llama-model">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;the financial bank is located on river bank&#34;</span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer(text, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span>token_ids <span style="color:#f92672">=</span> inputs<span style="color:#f92672">.</span>input_ids[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>convert_ids_to_tokens(token_ids)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> inputs<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#34;cuda&#34;</span>)
</span></span><span style="display:flex;"><span>    outputs <span style="color:#f92672">=</span> model(<span style="color:#f92672">**</span>inputs)
</span></span></code></pre></div><pre><code>`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=&quot;eager&quot;` when loading the model.
</code></pre>
<p>Get The Attention Matrix from the Outputs, we can see the below dimensions of the <code>attention_matrix</code> of length 28 and infer that there are 28 attention layers which we can also see from model architecture &amp; each layer&rsquo;s attention matrix is of shape (1,24,9,9) - This is because Llama Model has 24 Heads (This refers to Multi Head attention) and sequence length
of tokens that we passed is of length 9 hence the dimension of each head is 9*9</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attention_matrix <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>attentions
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34; Number of Attention Matrices == Number of Layers</span><span style="color:#e6db74">{</span>len(attention_matrix)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34; Shape of Each Attention Matrix </span><span style="color:#e6db74">{</span>attention_matrix[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code> Number of Attention Matrices == Number of Layers28
 Shape of Each Attention Matrix torch.Size([1, 24, 9, 9])
</code></pre>
<p>Get the last Layer from the Attention matrix and draw a heatmap for each head, as the Llama model we are referring has 24 heads below code snippet , will produce 24 heatmaps for each heads but due to lack of space and to keep it simple we will look at one of the heads , which captures the context of the same token <code>bank</code> according to its position and context</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">24</span>, <span style="color:#ae81ff">1</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">200</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, ax <span style="color:#f92672">in</span> enumerate(axes<span style="color:#f92672">.</span>flat):
</span></span><span style="display:flex;"><span>    sns<span style="color:#f92672">.</span>heatmap(attention_matrix[<span style="color:#ae81ff">27</span>][<span style="color:#ae81ff">0</span>][i]<span style="color:#f92672">.</span>cpu(), ax<span style="color:#f92672">=</span>ax, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;viridis&#34;</span>,annot<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,fmt<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;.2f&#34;</span>,xticklabels<span style="color:#f92672">=</span>tokenizer<span style="color:#f92672">.</span>convert_ids_to_tokens(inputs[<span style="color:#e6db74">&#34;input_ids&#34;</span>][<span style="color:#ae81ff">0</span>]),yticklabels<span style="color:#f92672">=</span>tokenizer<span style="color:#f92672">.</span>convert_ids_to_tokens(inputs[<span style="color:#e6db74">&#34;input_ids&#34;</span>][<span style="color:#ae81ff">0</span>]) )
</span></span><span style="display:flex;"><span>    ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Head </span><span style="color:#e6db74">{</span>i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,fontdict<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;fontsize&#39;</span>:<span style="color:#ae81ff">25</span>})
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img alt="png" loading="lazy" src="/images/transformers/Attentions/multihead_attention.png"></p>
<h1 id="focus-on-one-of-attention-head">Focus on One of Attention Head<a hidden class="anchor" aria-hidden="true" href="#focus-on-one-of-attention-head">#</a></h1>
<p><img alt="image" loading="lazy" src="https://github.com/user-attachments/assets/a08027b7-2a41-4578-b437-2f4f0d84a65a"></p>
<p><img alt="image" loading="lazy" src="https://github.com/user-attachments/assets/d3eb0e3d-7e84-420f-a070-92eb634b45ff"></p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/"></a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
