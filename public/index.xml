<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>My Learnings on My ML Portfolio</title>
    <link>http://localhost:55868/</link>
    <description>Recent content in My Learnings on My ML Portfolio</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:55868/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://localhost:55868/transformer/attention/attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/transformer/attention/attention/</guid>
      <description>&lt;h1 id=&#34;evolution-of-attention-mechanism&#34;&gt;Evolution of Attention Mechanism&lt;/h1&gt;
&lt;p&gt;Attention was firts introduced as part of seq-to-seq (Encoder-Decoder) models in the domain of Neural Machine Translation to translate text from one language to other language. Initial Architectures of encoder-decoder models were composed of encoder and decoder both are RNN&amp;rsquo;s, it is also possible to combine both simple RNN as part of encoder and GRU or LSTM for decoder , encoder takes sentence in source language as input and generates context vector of fixed lentgh , which would be passed as input to Decoder , Decoder takes the context vector and tries to map it to corresponding word or text in target language, this has few limitations as the single context vector generated by the encoder RNN could not capture the entire meaning of the sentence in source language which resulted less accurate results, especially as the length of the sequence grows the accuracy drops.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:55868/transformer/attention/multihead_attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/transformer/attention/multihead_attention/</guid>
      <description>&lt;h1 id=&#34;multihead-attention&#34;&gt;MultiHead Attention&lt;/h1&gt;
&lt;p&gt;The Multi Head Attention is an extension to Self Attention, while the Self Attentin defined in Transfomers helps us to overcome limitations faced by RNN&amp;rsquo;s, if we look at the above pitcutre we are calculating the attention over all heads of Llama Model , Multi Head Attention helps us to attend diferent aspects of elements in a sequence, in such case single weighted average is not
good option, to understand different aspects we divide the Query, Key &amp;amp; Value matrices to different Heads and calculate the attention scores of each head, to calculate attention for each head
we apply the same approach mentioned above,after the attention scores of each head are calculated we concatenate the attention scores of all the heads, this approach yeilds better results than
finding the attention as a whole, during this process the weight matrices that are split are learned for each head.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:55868/transformer/positionalembeddings/rope-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/transformer/positionalembeddings/rope-2/</guid>
      <description>&lt;h1 id=&#34;revisiting-positional-embeddings&#34;&gt;Revisiting Positional Embeddings&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When we were discussing about Positional Embeddings mentioned in
foundational paper &amp;ldquo;Attention is All You Need&amp;rdquo; , we got to know the
importance of positional encodings , also we got to know the two different approaches &lt;code&gt;Learned&lt;/code&gt; &amp;amp; &lt;code&gt;Fixed&lt;/code&gt; though the paper preferred
fixed positional embeddings over learned embeddings, the later results and metrics showed that fixed positional embeddings could not
accurately process the relation ship between different words or tokens , especially when the sequence whose length is more than sequence
encoutered during training.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:55868/transformer/positionalembeddings/rope/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/transformer/positionalembeddings/rope/</guid>
      <description>&lt;h1 id=&#34;rotatory-positional-embeddings&#34;&gt;Rotatory-Positional-Embeddings&lt;/h1&gt;
&lt;p&gt;Original Paper:- &lt;a href=&#34;https://arxiv.org/abs/2104.09864&#34;&gt;https://arxiv.org/abs/2104.09864&lt;/a&gt;
&lt;a href=&#34;https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26/&#34;&gt;https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26/&lt;/a&gt;
&lt;a href=&#34;https://aiexpjourney.substack.com/p/an-in-depth-exploration-of-rotary-position-embedding-rope-ac351a45c794&#34;&gt;https://aiexpjourney.substack.com/p/an-in-depth-exploration-of-rotary-position-embedding-rope-ac351a45c794&lt;/a&gt;
&lt;a href=&#34;https://medium.com/@DataDry/decoding-rotary-positional-embeddings-rope-the-secret-sauce-for-smarter-transformers-193cbc01e4ed&#34;&gt;https://medium.com/@DataDry/decoding-rotary-positional-embeddings-rope-the-secret-sauce-for-smarter-transformers-193cbc01e4ed&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When we were discussing about Positional Embeddings mentioned in foundational paper &amp;ldquo;Attention is All You Need&amp;rdquo; , we got to know the
importance of positional encodings , also we got to know the two different approaches &lt;code&gt;Learned&lt;/code&gt; &amp;amp; &lt;code&gt;Fixed&lt;/code&gt; though the paper preferred
fixed positional embeddings over learned embeddings, the later results and metrics showed that fixed positional embeddings could not
accurately process the relation ship between different words or tokens , especially when the sequence whose length is more than sequence
encoutered during training.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CUDA Programming Flow</title>
      <link>http://localhost:55868/cuda/cuda-kernel-workflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/cuda/cuda-kernel-workflow/</guid>
      <description></description>
    </item>
    <item>
      <title>Dataset Preparation</title>
      <link>http://localhost:55868/transformer/prepare_dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/transformer/prepare_dataset/</guid>
      <description></description>
    </item>
    <item>
      <title>Distributed Training</title>
      <link>http://localhost:55868/training/distributed-training/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/training/distributed-training/</guid>
      <description></description>
    </item>
    <item>
      <title>GPU Thread Hierarchy and Indexing</title>
      <link>http://localhost:55868/cuda/thread-indexing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/cuda/thread-indexing/</guid>
      <description></description>
    </item>
    <item>
      <title>Naive Matrix Multiplication</title>
      <link>http://localhost:55868/cuda/naive_gemm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/cuda/naive_gemm/</guid>
      <description></description>
    </item>
    <item>
      <title>Pytorch DDP Setup -Multi Node</title>
      <link>http://localhost:55868/training/pytorch_dpp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/training/pytorch_dpp/</guid>
      <description></description>
    </item>
    <item>
      <title>Tiled GEMM</title>
      <link>http://localhost:55868/cuda/tile_gemm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:55868/cuda/tile_gemm/</guid>
      <description></description>
    </item>
  </channel>
</rss>
