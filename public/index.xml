<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>My Learnings on </title>
    <link>http://localhost:1313/</link>
    <description>Recent content in My Learnings on </description>
    <generator>Hugo -- 0.147.7</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://localhost:1313/posts/transformer/positionalembeddings/rope/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/transformer/positionalembeddings/rope/</guid>
      <description>&lt;h1 id=&#34;rotatory-positional-embeddings&#34;&gt;Rotatory-Positional-Embeddings&lt;/h1&gt;
&lt;p&gt;Original Paper:- &lt;a href=&#34;https://arxiv.org/abs/2104.09864&#34;&gt;https://arxiv.org/abs/2104.09864&lt;/a&gt;
&lt;a href=&#34;https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26/&#34;&gt;https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26/&lt;/a&gt;
&lt;a href=&#34;https://aiexpjourney.substack.com/p/an-in-depth-exploration-of-rotary-position-embedding-rope-ac351a45c794&#34;&gt;https://aiexpjourney.substack.com/p/an-in-depth-exploration-of-rotary-position-embedding-rope-ac351a45c794&lt;/a&gt;
&lt;a href=&#34;https://medium.com/@DataDry/decoding-rotary-positional-embeddings-rope-the-secret-sauce-for-smarter-transformers-193cbc01e4ed&#34;&gt;https://medium.com/@DataDry/decoding-rotary-positional-embeddings-rope-the-secret-sauce-for-smarter-transformers-193cbc01e4ed&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When we were discussing about Positional Embeddings mentioned in foundational paper &amp;ldquo;Attention is All You Need&amp;rdquo; , we got to know the
importance of positional encodings , also we got to know the two different approaches &lt;code&gt;Learned&lt;/code&gt; &amp;amp; &lt;code&gt;Fixed&lt;/code&gt; though the paper preferred
fixed positional embeddings over learned embeddings, the later results and metrics showed that fixed positional embeddings could not
accurately process the relation ship between different words or tokens , especially when the sequence whose length is more than sequence
encoutered during training.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CUDA Programming Flow</title>
      <link>http://localhost:1313/posts/cuda/cuda-kernel-workflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/cuda/cuda-kernel-workflow/</guid>
      <description></description>
    </item>
    <item>
      <title>Dataset Preparation</title>
      <link>http://localhost:1313/posts/transformer/prepare_dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/transformer/prepare_dataset/</guid>
      <description></description>
    </item>
    <item>
      <title>Distributed Training</title>
      <link>http://localhost:1313/posts/training/distributed-training/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/training/distributed-training/</guid>
      <description></description>
    </item>
    <item>
      <title>Evolution of Attention</title>
      <link>http://localhost:1313/posts/transformer/attention/attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/transformer/attention/attention/</guid>
      <description></description>
    </item>
    <item>
      <title>Flash Attention</title>
      <link>http://localhost:1313/posts/transformer/attention/flash_attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/transformer/attention/flash_attention/</guid>
      <description></description>
    </item>
    <item>
      <title>GPU Thread Hierarchy and Indexing</title>
      <link>http://localhost:1313/posts/cuda/thread-indexing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/cuda/thread-indexing/</guid>
      <description></description>
    </item>
    <item>
      <title>Naive Matrix Multiplication</title>
      <link>http://localhost:1313/posts/cuda/naive_gemm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/cuda/naive_gemm/</guid>
      <description></description>
    </item>
    <item>
      <title>Peek Into MultiHead Attention</title>
      <link>http://localhost:1313/posts/transformer/attention/multihead_attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/transformer/attention/multihead_attention/</guid>
      <description></description>
    </item>
    <item>
      <title>Pytorch DDP Setup -Multi Node</title>
      <link>http://localhost:1313/posts/training/pytorch_dpp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/training/pytorch_dpp/</guid>
      <description></description>
    </item>
    <item>
      <title>Rotary Positional Embeddings</title>
      <link>http://localhost:1313/posts/transformer/positionalembeddings/rope-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/transformer/positionalembeddings/rope-2/</guid>
      <description></description>
    </item>
    <item>
      <title>Tiled GEMM</title>
      <link>http://localhost:1313/posts/cuda/tile_gemm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/cuda/tile_gemm/</guid>
      <description></description>
    </item>
  </channel>
</rss>
