<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Attention Mechanisms on </title>
    <link>http://localhost:1313/transformer/attention/</link>
    <description>Recent content in Attention Mechanisms on </description>
    <generator>Hugo -- 0.147.7</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/transformer/attention/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://localhost:1313/transformer/attention/attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/transformer/attention/attention/</guid>
      <description>&lt;h1 id=&#34;evolution-of-attention-mechanism&#34;&gt;Evolution of Attention Mechanism&lt;/h1&gt;
&lt;p&gt;Attention was firts introduced as part of seq-to-seq (Encoder-Decoder) models in the domain of Neural Machine Translation to translate text from one language to other language. Initial Architectures of encoder-decoder models were composed of encoder and decoder both are RNN&amp;rsquo;s, it is also possible to combine both simple RNN as part of encoder and GRU or LSTM for decoder , encoder takes sentence in source language as input and generates context vector of fixed lentgh , which would be passed as input to Decoder , Decoder takes the context vector and tries to map it to corresponding word or text in target language, this has few limitations as the single context vector generated by the encoder RNN could not capture the entire meaning of the sentence in source language which resulted less accurate results, especially as the length of the sequence grows the accuracy drops.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/transformer/attention/multihead_attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/transformer/attention/multihead_attention/</guid>
      <description>&lt;h1 id=&#34;multihead-attention&#34;&gt;MultiHead Attention&lt;/h1&gt;
&lt;p&gt;The Multi Head Attention is an extension to Self Attention, while the Self Attentin defined in Transfomers helps us to overcome limitations faced by RNN&amp;rsquo;s, if we look at the above pitcutre we are calculating the attention over all heads of Llama Model , Multi Head Attention helps us to attend diferent aspects of elements in a sequence, in such case single weighted average is not
good option, to understand different aspects we divide the Query, Key &amp;amp; Value matrices to different Heads and calculate the attention scores of each head, to calculate attention for each head
we apply the same approach mentioned above,after the attention scores of each head are calculated we concatenate the attention scores of all the heads, this approach yeilds better results than
finding the attention as a whole, during this process the weight matrices that are split are learned for each head.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
